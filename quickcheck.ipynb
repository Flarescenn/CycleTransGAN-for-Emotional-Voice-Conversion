{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention layer output shape: torch.Size([2, 10, 1024])\n",
      "tensor([[[-0.1422, -0.0356, -0.0091,  ..., -0.0275, -0.1920,  0.0517],\n",
      "         [ 0.0030, -0.0731, -0.2004,  ...,  0.0390, -0.0783, -0.0441],\n",
      "         [-0.0606, -0.1078, -0.0843,  ..., -0.0563, -0.0026, -0.0169],\n",
      "         ...,\n",
      "         [-0.1833,  0.0234, -0.1320,  ...,  0.0100, -0.0881, -0.0189],\n",
      "         [-0.0416, -0.0453, -0.0318,  ..., -0.0214, -0.0495, -0.0151],\n",
      "         [-0.0712, -0.0732, -0.0458,  ...,  0.0342, -0.1147,  0.0081]],\n",
      "\n",
      "        [[ 0.0795, -0.2990,  0.0414,  ..., -0.0633, -0.1813,  0.0455],\n",
      "         [ 0.0201, -0.1773,  0.0545,  ...,  0.0185, -0.1866,  0.0798],\n",
      "         [ 0.1127, -0.3299,  0.1252,  ..., -0.0645, -0.2336, -0.0304],\n",
      "         ...,\n",
      "         [ 0.0591, -0.2137, -0.0392,  ..., -0.0780, -0.2110, -0.0872],\n",
      "         [ 0.0594, -0.2677,  0.0189,  ...,  0.0406, -0.2667, -0.1104],\n",
      "         [ 0.0672, -0.3054,  0.1785,  ..., -0.0820, -0.1941, -0.0598]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer import AttentionLayer  # Assuming your implementation is in transformer.py\n",
    "\n",
    "# Define batch size, sequence length, and model dimensions\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "hidden_size = 1024  # Hidden size from your transformer model\n",
    "\n",
    "# Number of attention heads and size per head\n",
    "num_attention_heads = 4\n",
    "size_per_head = hidden_size // num_attention_heads\n",
    "\n",
    "# Create random input tensors (simulating an input to the attention layer)\n",
    "input_tensor = torch.randn(batch_size, seq_length, hidden_size)\n",
    "\n",
    "# Create the Attention Layer instance\n",
    "attention_layer = AttentionLayer(num_attention_heads=num_attention_heads, size_per_head=size_per_head)\n",
    "\n",
    "# Forward pass through the attention layer\n",
    "output = attention_layer(input_tensor, input_tensor)  # Query and key are the same for self-attention\n",
    "\n",
    "# Check output shape\n",
    "print(f\"Attention layer output shape: {output.shape}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer output shape: torch.Size([2, 10, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer import Transformer\n",
    "\n",
    "# Define the test parameters\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "hidden_size = 1024  # Should match the model's hidden size\n",
    "\n",
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(batch_size, seq_length, hidden_size)\n",
    "\n",
    "# Define other parameters for the transformer layer\n",
    "num_hidden_layers = 6\n",
    "num_attention_heads = 4\n",
    "intermediate_size = 2048\n",
    "\n",
    "# Instantiate the transformer\n",
    "transformer = Transformer(\n",
    "    hidden_size=hidden_size,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    intermediate_size=intermediate_size\n",
    ")\n",
    "\n",
    "# Perform the forward pass\n",
    "output = transformer(input_tensor)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(f\"Transformer output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.037908673286438\n",
      "Input tensor gradients: tensor([[[-2.5584e-05,  9.1630e-06,  4.3835e-05,  ..., -3.5799e-05,\n",
      "          -1.8641e-06, -5.5898e-05],\n",
      "         [-3.8660e-05,  2.2877e-05,  2.1538e-05,  ..., -3.1388e-05,\n",
      "           1.9090e-05, -3.3681e-05],\n",
      "         [-1.6117e-05,  2.4816e-05,  6.8442e-05,  ..., -1.6960e-05,\n",
      "           1.4013e-05, -3.7118e-05],\n",
      "         ...,\n",
      "         [-2.1360e-05,  2.0879e-05,  2.5869e-05,  ..., -3.3268e-05,\n",
      "           1.9415e-05, -2.6377e-05],\n",
      "         [-6.1441e-06,  1.1794e-05,  6.2522e-05,  ..., -2.0422e-05,\n",
      "           8.8140e-06, -3.0288e-05],\n",
      "         [-1.9210e-05,  2.8447e-05,  2.7604e-05,  ..., -2.5945e-05,\n",
      "           2.7842e-05, -3.4455e-05]],\n",
      "\n",
      "        [[-1.7136e-06,  2.1871e-05,  2.5296e-05,  ..., -1.9575e-05,\n",
      "           3.2393e-05, -2.5606e-05],\n",
      "         [-1.8344e-05, -1.6368e-07,  1.9154e-05,  ..., -1.9563e-06,\n",
      "           1.9994e-05,  1.3611e-06],\n",
      "         [-7.1946e-06,  8.0717e-07, -7.1339e-06,  ..., -1.3799e-05,\n",
      "           2.6126e-05, -2.5243e-05],\n",
      "         ...,\n",
      "         [ 7.7143e-06,  3.2686e-05, -3.5622e-07,  ..., -1.5602e-05,\n",
      "           3.5946e-06, -1.2277e-05],\n",
      "         [-8.0502e-06,  1.8265e-05, -1.1331e-06,  ..., -1.7814e-05,\n",
      "           1.9525e-05, -1.3493e-05],\n",
      "         [-4.7407e-06,  2.6597e-05,  1.0376e-05,  ..., -3.0931e-05,\n",
      "           2.5540e-05, -4.2935e-05]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer import AttentionLayer  # Assuming your implementation is in transformer.py\n",
    "\n",
    "# Define test parameters\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "hidden_size = 1024\n",
    "\n",
    "# Number of attention heads and size per head\n",
    "num_attention_heads = 4\n",
    "size_per_head = hidden_size // num_attention_heads\n",
    "\n",
    "# Create random input tensors and targets\n",
    "input_tensor = torch.randn(batch_size, seq_length, hidden_size, requires_grad=True)  # Set requires_grad=True to track gradients\n",
    "target_tensor = torch.randn(batch_size, seq_length, hidden_size)\n",
    "\n",
    "# Create the Attention Layer instance\n",
    "attention_layer = AttentionLayer(num_attention_heads=num_attention_heads, size_per_head=size_per_head)\n",
    "\n",
    "# Define a loss function (e.g., MSELoss)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Forward pass through the attention layer\n",
    "output = attention_layer(input_tensor, input_tensor)  # Self-attention (query, key are the same)\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(output, target_tensor)\n",
    "print(f\"Loss: {loss}\")\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients for the input tensor\n",
    "print(f\"Input tensor gradients: {input_tensor.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
